#   Multi-Modal RAG Assistant ğŸ¤–

   This project implements a Multi-Modal Retrieval-Augmented Generation (RAG) System that can process and retrieve information from a knowledge base containing various types of content such as videos ğŸ¥, documents ğŸ“„, and images ğŸ–¼ï¸. The system is designed to provide text-based responses to user queries in a conversational manner. ğŸ—£ï¸

   ##   âœ¨ Features

   -   Processes multi-modal data (documents ğŸ“„, images ğŸ–¼ï¸, videos ğŸ¥) [cite: 4, 5]
   -   Accurately answers questions related to the knowledge base [cite: 5]
   -   Simple user interface for interacting with the AI assistant [cite: 6]
   -   Optional: Conversational memory ğŸ§  [cite: 7]

   ##   ğŸ“¥ Requirements

   -   Input: A folder containing images ğŸ–¼ï¸, videos ğŸ¥, and documents (PDF ğŸ“„ or Word) related to a specific domain or topic. [cite: 4]

   ##   ğŸ“¦ Output Deliverables

   -   GitHub repository with all code and documentation ğŸ’» [cite: 7]
   -   Folder containing the input data for the AI assistant to process ğŸ“‚ [cite: 7]
   -   Video recording explaining the approach and code walkthrough ğŸ“¹ [cite: 8]

   ##   ğŸ† Evaluation Criteria

   -   Technical Depth: Efficient handling of multi-modal data, retrieval mechanisms, and response generation. [cite: 9]
   -   Code Quality: Well-structured, maintainable, and documented code. [cite: 10]

   ##   ğŸ‘¨â€ğŸ’» Code Overview

   ###   `streamlit_app.py`

   This file contains the main Streamlit application for the Multi-Modal RAG assistant. ğŸš€

   ####   `MultiModalRAGApp` Class

   -   `__init__(self)`: Initializes the application, sets up the session state, initializes RAG components, and builds the user interface. âš™ï¸
   -   `_setup_session(self)`: Sets up Streamlit session state variables for conversation ID, messages ğŸ’¬, showing sources, and storing last sources. ğŸ”‘
   -   `_initialize_rag_components(self)`: Initializes the vector store ğŸ“š, retriever ğŸ”, and LLM ğŸ§ .
   -   `_build_ui(self)`: Builds the Streamlit user interface, displaying chat messages ğŸ’¬ and providing options. ğŸ¨
   -   `_handle_query(self, query: str)`: Handles user queries â“, retrieves relevant information ğŸ”, generates a response using the LLM ğŸ§ , and displays the response. ğŸ’¡

   ####   Main Execution

   -   The `main()` function creates and runs the `MultiModalRAGApp`. â–¶ï¸

   ###   `process_data.py`

   This script handles the processing of multi-modal data and the creation of the vector database. ğŸ’¾

   ####   Functions

   -   `check_gpu()`: Checks for GPU availability and prints GPU information. ğŸ’»
   -   `clear_memory()`: Clears memory (CPU and GPU). ğŸ§¹
   -   `process_all_data(data_dir: str, output_dir: str, batch_size: int = 8)`: Processes documents ğŸ“„, images ğŸ–¼ï¸, and videos ğŸ¥ in a directory and saves the processed data. ğŸ“
   -   `create_vector_database(processed_paths: dict, vector_db_path: str, batch_size: int = 1024)`: Creates a vector database from the processed data. ğŸ—„ï¸
   -   `optimize_gpu_env()`: Optimizes the GPU environment by setting environment variables. âš™ï¸
   -   `setup_colab()`: Sets up the environment in Google Colab (installs ffmpeg). â˜ï¸
   -   `main()`: Parses command-line arguments, sets up the environment, processes data, and creates the vector database. ğŸš€

   ###   `config.py`

   This file contains configuration settings for the application. âš™ï¸

   ####   Configuration Variables

   -   `GROQ_API_KEY`: API key for the Groq LLM. ğŸ”‘
   -   `DATA_DIR`, `PROCESSED_DATA_DIR`, `EMBEDDINGS_DIR`, `TEXT_CHUNKS_DIR`, `MEDIA_METADATA_DIR`: Directories for data storage. ğŸ“
   -   `LLM_MODEL`, `TEMPERATURE`, `MAX_TOKENS`: LLM configuration. ğŸ§ 
   -   `VECTOR_DB_TYPE`, `VECTOR_DB_PATH`: Vector database configuration. ğŸ—„ï¸
   -   `EMBEDDING_MODEL`, `EMBEDDING_DIMENSION`: Embedding model configuration. ğŸ§®
   -   `CHUNK_SIZE`, `CHUNK_OVERLAP`: Chunking parameters for documents. âœ‚ï¸
   -   `TOP_K_RESULTS`: Number of results to retrieve. ğŸ’¯
   -   `STREAMLIT_TITLE`, `STREAMLIT_DESCRIPTION`: Streamlit app appearance. ğŸ¨
   -   `OCR_ENGINE`: OCR engine to use. ğŸ‘ï¸
   -   `AUDIO_EXTRACTION_METHOD`: Method for audio extraction from videos. ğŸ§
   -   `VIDEO_FRAME_SAMPLE_RATE`: Frame sample rate for videos. ğŸï¸

   ##   ğŸš€ Running Instructions

   1.  **Prepare your data:** ğŸ“‚
       -   Place your documents ğŸ“„, images ğŸ–¼ï¸, and videos ğŸ¥ in the `Data` directory.
   2.  **Install dependencies:** ğŸ“¦
       -   Run `pip install -r requirements.txt` to install the required Python packages.
   3.  **Set the Groq API key:** ğŸ”‘
       -   Ensure you have a Groq API key and set it as an environment variable named `GROQ_API_KEY`.
   4.  **Process the data and create the vector database:** ğŸ’¾

       -   Run `python process_data.py --data-dir /path/to/Data --output-dir /path/to/processed_data --vector-db-path /path/to/vector_db` ğŸƒ
       -   Replace `/path/to/Data`, `/path/to/processed_data`, and `/path/to/vector_db` with the actual paths. ğŸ“
       -   Optional arguments: âš™ï¸
           -   `--skip-processing`: Skips data processing if it's already done. â©
           -   `--batch-size`: Sets the batch size for processing. ğŸ”¢
           -   `--embedding-batch-size`: Sets the batch size for creating embeddings. ğŸ§®
           -   `--colab-setup`: Use this flag if running in Google Colab. â˜ï¸
   5.  **Run the Streamlit application:** ğŸš€
       -   Run `streamlit run streamlit_app.py`
   6.  **Interact with the assistant:** ğŸ—£ï¸
       -   Open the Streamlit app in your browser and start asking questions about your data! â“
